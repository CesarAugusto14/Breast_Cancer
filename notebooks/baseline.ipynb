{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a CNN with the 256x256 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "# Import libraries for plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# All the figures will be 8,8\n",
    "plt.rcParams['figure.figsize'] = [8, 8]\n",
    "\n",
    "# Set the font to times and 18\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.size'] = 18\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Baseline Model\n",
    "\n",
    "In this notebook, I will be training a shallow CNN with just the right breast MLO view using 256x256 images.\n",
    "\n",
    "### Dataset Object\n",
    "\n",
    "The following Dataset object was created on purpose, to show what is the issue with the class unbalance on this kind of problems. Also, it is uploading ALL THE IMAGES, without caring about the modality of the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In this cell, I will be declaring the Dataset class, which will be used to load the images and the labels.\n",
    "\"\"\"\n",
    "df = pd.read_csv('../data/train.csv')\n",
    "# df.sort_values(by='patient_id', inplace=True)\n",
    "path_images = '../data/256_images/'\n",
    "list_images = sorted(os.listdir(path_images))\n",
    "\n",
    "# I will need a torch Dataset to load the images and the labels.\n",
    "class DatasetBreastWrong(torch.utils.data.Dataset):\n",
    "    \"\"\"The labels will be available through df.cancer, whereas the images will be loaded from the /data/256_images folder.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, path_patients, transform=None):\n",
    "        self.df = df\n",
    "        self.path_patients = path_patients\n",
    "        self.transform = transform\n",
    "        self.list_patients = sorted(os.listdir(path_patients))\n",
    "        self.list_patients = [x for x in self.list_patients if x != '.DS_Store']\n",
    "        # From the list of patients, I will need to get access to the images\n",
    "        self.list_images = []\n",
    "        for patient in self.list_patients:\n",
    "            for image in sorted(os.listdir(path_images + patient)):\n",
    "                if image != '.DS_Store':\n",
    "                    self.list_images.append(patient + '/' + image)\n",
    "        self.list_images = sorted(self.list_images)\n",
    "        self.labels = self.df.cancer.values\n",
    "        self.labels = self.labels.astype(np.float32)\n",
    "        self.labels = torch.from_numpy(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        # Get the image\n",
    "        path_image = self.path_patients + self.list_images[idx]\n",
    "        image = plt.imread(path_image)\n",
    "        image = image.astype(np.float32)\n",
    "        image = torch.from_numpy(image)\n",
    "        # image = image.permute(2, 0, 1)\n",
    "        # Get the label it should be either 0 or 1\n",
    "        label = self.labels[idx]\n",
    "        sample = {'image': image, 'label': label}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "\n",
    "dataset_wrong = DatasetBreastWrong(df, path_images)\n",
    "# print(dataset_wrong.list_images)\n",
    "# Check if it is working\n",
    "sample = dataset_wrong[5]\n",
    "sample2 = dataset_wrong[20]\n",
    "print(sample['image'].shape)\n",
    "print(sample['label'])\n",
    "\n",
    "# Show the image as a numpy array int32\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(sample['image'].numpy().astype(np.int32), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(sample2['image'].numpy().astype(np.int32), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset with just one modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In this cell, I will create the proper Dataset object, using just one view like CC or MLO.\n",
    "\n",
    "The dataset will consider the heavy inbalance of the data, so I will downsample the majority class.\n",
    "\"\"\"\n",
    "class DatasetBreastDownsample(torch.utils.data.Dataset):\n",
    "    \"\"\"The labels will be available through df.cancer, whereas the images will be loaded from the /data/256_images folder.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, path_patients, transform=None, view='CC', breast = 'L'):\n",
    "        # From the dataframe, I will need to remove the images that are not the view I want. \n",
    "        # I will also need to remove the images that are not the breast I want.\n",
    "        # Finally, I need to downsample the majority class. \n",
    "        self.df = df\n",
    "        self.path_patients = path_patients\n",
    "        self.transform = transform\n",
    "        self.view = view\n",
    "        self.breast = breast\n",
    "         # Working with the dataframe\n",
    "        self.df = self.df[self.df.view == self.view]\n",
    "        self.df = self.df[self.df.laterality == self.breast]\n",
    "\n",
    "        # I need to randomly select the patients that are in the majority class\n",
    "        self.list_patients_majority = np.unique(self.df.patient_id[(self.df.cancer == 0).values])\n",
    "        self.list_patients_minority = np.unique(self.df.patient_id[(self.df.cancer == 1).values])\n",
    "        # I will need to downsample the majority class\n",
    "\n",
    "        self.list_patients_majority = np.random.choice(self.list_patients_majority, \n",
    "                                                       size=len(self.list_patients_minority), \n",
    "                                                       replace=False)\n",
    "        \n",
    "        self.list_patients = np.concatenate((self.list_patients_majority, \n",
    "                                             self.list_patients_minority))\n",
    "       \n",
    "        # Reduce the dataframe to only have the list of patients\n",
    "        self.df = self.df[self.df.patient_id.isin(self.list_patients)]\n",
    "        self.df.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "        \n",
    "        # From the df, I will need to construct the list of paths for the desired images, the format will be patient_id/image_id.dcm\n",
    "        self.list_images = []\n",
    "        for patient in self.list_patients:\n",
    "            for image in self.df.image_id[self.df.patient_id == patient].values:\n",
    "                self.list_images.append(str(patient) + '/' + str(image) + '.jpg')\n",
    "        self.list_images = sorted(self.list_images)\n",
    "        self.labels = self.df.cancer.values\n",
    "        self.labels = self.labels.astype(np.float32)\n",
    "        self.labels = torch.from_numpy(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        # Get the image\n",
    "        path_image = self.path_patients + self.list_images[idx]\n",
    "        image = plt.imread(path_image)\n",
    "        image = image.astype(np.float32)\n",
    "        image = torch.from_numpy(image)\n",
    "        # image = image.permute(2, 0, 1)\n",
    "        # Get the label it should be either 0 or 1\n",
    "        label = self.labels[idx]\n",
    "        sample = {'image': image, 'label': label}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "\n",
    "dset_trial = DatasetBreastDownsample(df, path_images, view='CC', breast='L')\n",
    "# Check if it is working\n",
    "sample = dset_trial[0]\n",
    "sample2 = dset_trial[20]\n",
    "print(sample['image'].shape)\n",
    "print(sample['label'])\n",
    "\n",
    "# Show the image as a numpy array int32\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(sample['image'].numpy().astype(np.int32))\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(sample2['image'].numpy().astype(np.int32))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dset_trial.df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-validation-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation-test split\n",
    "train_size = int(0.8 * len(dset_trial))\n",
    "val_size = int(0.1 * len(dset_trial))\n",
    "test_size = len(dset_trial) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dset_trial, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "# Create the dataloaders\n",
    "size_batch = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=size_batch, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=size_batch, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=size_batch, shuffle=True)\n",
    "\n",
    "# Check if it is working\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    print(i_batch, sample_batched['image'].size(), sample_batched['label'])\n",
    "    if i_batch == 3:\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that now we have a balanced dataset. With similar probabilities to find 1s and 0 labels. However, this is a very small set, which might be extremely hard to use for any Deep Learning Model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model (2D - Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create 2D convolutional neural network for the classification task of the cancer images (binary classification), based on images 3x256x256. I will be using a mini batch size of 32\"\"\"\n",
    "\n",
    "# Create a model\n",
    "model = nn.Sequential()\n",
    "model.add_module('conv1', nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 3, stride = 1, padding = 1))\n",
    "model.add_module('relu1', nn.ReLU())\n",
    "model.add_module('pool1', nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "model.add_module('conv2', nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 3, stride = 1, padding = 1))\n",
    "model.add_module('relu2', nn.ReLU())\n",
    "model.add_module('pool2', nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "model.add_module('conv3', nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, stride = 1, padding = 1))\n",
    "model.add_module('relu3', nn.ReLU())\n",
    "model.add_module('pool3', nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "# flatten\n",
    "model.add_module('flatten', nn.Flatten())\n",
    "model.add_module('fc1', nn.Linear(in_features = 32768, out_features = 512))\n",
    "model.add_module('relu4', nn.ReLU())\n",
    "model.add_module('dropout', nn.Dropout(p = 0.5))\n",
    "model.add_module('fc2', nn.Linear(in_features = 512, out_features = 256))\n",
    "model.add_module('relu5', nn.ReLU())\n",
    "model.add_module('dropout', nn.Dropout(p = 0.5))\n",
    "model.add_module('fc3', nn.Linear(in_features = 256, out_features = 1))\n",
    "model.add_module('sigmoid', nn.Sigmoid())\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "# Check if it is working\n",
    "# sample = dataset[5]\n",
    "# sample2 = dataset[20]\n",
    "# print(sample['image'].shape)\n",
    "# print(sample['label'])\n",
    "# print(model(sample['image'].unsqueeze(0)).item())\n",
    "# print(model(sample2['image'].unsqueeze(0)).item())\n",
    "\n",
    "# Summary\n",
    "summary(model, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model \n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.BCELoss()\n",
    "# Define a loss function with l2 regularization\n",
    "# criterion = nn.BCELoss() + 0.01*torch.norm(model.fc1.weight, 2) + 0.01*torch.norm(model.fc2.weight, 2)\n",
    "\n",
    "# Define the optimizer with l2 regularization\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5, weight_decay=1, momentum=0.5)\n",
    "\n",
    "# Train the model\n",
    "def train(model, num_epochs, train_dl, valid_dl, track = False):\n",
    "    loss_hist_train = [0]*num_epochs\n",
    "    loss_hist_valid = [0]*num_epochs\n",
    "    accuracy_hist_train = [0]*num_epochs\n",
    "    accuracy_hist_valid = [0]*num_epochs\n",
    "    for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        for i, sample_batched in enumerate(train_dl):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sample_batched['image'])\n",
    "            loss = criterion(outputs, sample_batched['label'].unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_hist_train[epoch] += loss.item()\n",
    "            accuracy_hist_train[epoch] += (outputs.round() == sample_batched['label'].unsqueeze(1)).sum().item()\n",
    "        loss_hist_train[epoch] /= len(train_dl)\n",
    "        accuracy_hist_train[epoch] /= len(train_dl.dataset)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, sample_batched in enumerate(valid_dl):\n",
    "                outputs = model(sample_batched['image'])\n",
    "                loss = criterion(outputs, sample_batched['label'].unsqueeze(1))\n",
    "                loss_hist_valid[epoch] += loss.item()\n",
    "                accuracy_hist_valid[epoch] += (outputs.round() == sample_batched['label'].unsqueeze(1)).sum().item()\n",
    "        loss_hist_valid[epoch] /= len(valid_dl)\n",
    "        accuracy_hist_valid[epoch] /= len(valid_dl.dataset)\n",
    "\n",
    "        # print loss and accuracy for the train set and validation set\n",
    "        if track:\n",
    "            print('Epoch [{}/{}], train_Loss: {:.4f}, train_acc: {:.2f}%'.format(epoch+1, num_epochs, loss_hist_train[epoch], accuracy_hist_train[epoch]*100))\n",
    "            print('val_loss: {:.4f}, val_acc: {:.2f}%'.format(loss_hist_valid[epoch], accuracy_hist_valid[epoch]*100))\n",
    "    return model, loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid\n",
    "num_epochs = 100\n",
    "\n",
    "model, loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid = train(model, num_epochs, train_loader, val_loader, track = True)\n",
    "\n",
    "# Plot the loss and accuracy for training\n",
    "plt.plot(loss_hist_train, label='Training loss')\n",
    "plt.plot(loss_hist_valid, label='Validation loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the loss and accuracy for validation\n",
    "plt.plot(accuracy_hist_train, label='Training accuracy')\n",
    "plt.plot(accuracy_hist_valid, label='Validation accuracy')\n",
    "plt.legend(frameon=False)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the validation accuracy is extremely unstable, and my best model had an accuracy on the validation set of 60%. Note also that ther training accuracy and the validation accuracy are extremely different, which suggests overfitting. \n",
    "\n",
    "Below, I am presenting the test loss and accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    loss_hist_test = 0\n",
    "    accuracy_hist_test = 0\n",
    "\n",
    "    for i, sample_batched in enumerate(test_loader):\n",
    "        outputs = model(sample_batched['image'])\n",
    "        loss = criterion(outputs, sample_batched['label'].unsqueeze(1))\n",
    "        loss_hist_test += loss.item()\n",
    "        accuracy_hist_test += (outputs.round() == sample_batched['label'].unsqueeze(1)).sum().item()\n",
    "    loss_hist_test /= len(test_loader)\n",
    "    accuracy_hist_test /= len(test_loader.dataset)\n",
    "\n",
    "    print('test_loss: {:.4f}, test_acc: {:.2f}%'.format(loss_hist_test, accuracy_hist_test*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "model.eval()\n",
    "import seaborn as sns\n",
    "with torch.no_grad():\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for i, sample_batched in enumerate(test_loader):\n",
    "        outputs = model(sample_batched['image'])\n",
    "        y_pred.extend(outputs.round().tolist())\n",
    "        y_true.extend(sample_batched['label'].tolist())\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(\"../results/confusion_baseline.png\")\n",
    "    \n",
    "    plt.show()                                                      \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# Compute the ROC curve\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for i, sample_batched in enumerate(test_loader):\n",
    "        outputs = model(sample_batched['image'])\n",
    "        y_pred.extend(outputs.tolist())\n",
    "        y_true.extend(sample_batched['label'].tolist())\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    print(\"AUC: {:.2f}\".format(auc))\n",
    "    plt.plot(fpr, tpr, label = f'AUC = {auc:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.title(\"ROC curve\")\n",
    "    plt.xlabel(\"False positive rate\")\n",
    "    plt.ylabel(\"True positive rate\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"../results/roc_baseline.png\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy for training\n",
    "plt.plot(loss_hist_train, label='Training loss')\n",
    "plt.plot(loss_hist_valid, label='Validation loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.title(\"Loss\")\n",
    "plt.savefig(\"../results/loss_baseline.jpg\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()\n",
    "\n",
    "# Plot the loss and accuracy for validation\n",
    "plt.plot(accuracy_hist_train, label='Training accuracy')\n",
    "plt.plot(accuracy_hist_valid, label='Validation accuracy')\n",
    "plt.legend(frameon=False)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.savefig(\"../results/accuracy_baseline.jpg\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), \"../results/baseline.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05e211caa9a0490faf5af5ecf5729daa8d730744e9220add6b63a443d81066b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
